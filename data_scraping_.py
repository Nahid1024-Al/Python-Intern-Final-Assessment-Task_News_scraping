# -*- coding: utf-8 -*-
"""data scraping .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vfuxEMclcwrggAiJrloIGxWOpQ-_Od1m
"""

import requests

from bs4 import BeautifulSoup

import json

def scrape_urls_and_data(category_url):
    # Send request to the webpage
    response = requests.get(category_url)
    response.encoding = 'utf-8'
    soup = BeautifulSoup(response.text, 'html.parser')

    # Base URL to prepend to relative URLs (if needed)
    base_url = "https://www.tbsnews.net"

    # List to store scraped data
    news_data = []

    # Find all articles (adjust this if the HTML structure is different)
    for article in soup.find_all('div', class_='views-field'):
        try:
            # Extract article title and URL
            title_tag = article.find('a')
            title = title_tag.get_text(strip=True)
            article_url = base_url + title_tag['href']  # Assuming the href is relative

            # Optional: Extract other data if available
            summary = article.find('div', class_='views-field-field-summary').get_text(strip=True) if article.find('div', class_='views-field-field-summary') else 'No summary available'
            published_date = article.find('span', class_='post-date').get_text(strip=True) if article.find('span', class_='post-date') else 'No date available'

            # Append the data to the list
            news_data.append({
                'title': title,
                'url': article_url,
                'summary': summary,
                'published_date': published_date
             })
        except Exception as e:
            print(f"Error scraping article: {e}") # Indented this line to be part of the except block

    return news_data

def save_to_json(data, filename='news_data.json'):
    # Save the scraped data into a JSON file
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

# URL of the category page you want to scrape
category_url = 'https://www.tbsnews.net/bangla/%E0%A6%85%E0%A6%B0%E0%A7%8D%E0%A6%A5%E0%A6%A8%E0%A7%80%E0%A6%A4%E0%A6%BF'

# Scrape data and save it into a JSON file
news_data = scrape_urls_and_data(category_url)
save_to_json(news_data)

print(f"Scraped {len(news_data)} articles and saved to news_data.json")

!pip install selenium

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
import json
import time

# ... (rest of your code)

def scrape_with_selenium(url):
    # Set up the WebDriver using Service
    service = Service(executable_path=r'/content/chromedriver.exe')  # Create a Service object
    driver = webdriver.Chrome(service=service)  # Pass the Service object to the constructor
     # Open the webpage
    driver.get(url)

    # Wait for the page to fully load
    time.sleep(5)  # Adjust the time depending on how long the page takes to load

    # Get the page source and parse it
    html = driver.page_source
    soup = BeautifulSoup(html, 'html.parser')

    # Close the WebDriver
    driver.quit()

    return soup
 # ... (rest of the function)

!chmod +x /content/chromedriver.exe  # Give execute permission to the chromedriver

def scrape_with_selenium(url):
    # Set up the WebDriver using Service
    # Update the executable_path to the correct location of your chromedriver
    service = Service(executable_path=r'/path/to/chromedriver')  # Create a Service object
    # Add options if needed, such as headless mode
    options = webdriver.ChromeOptions()
    # options.add_argument('--headless')  # Uncomment to run in headless mode
    driver = webdriver.Chrome(service=service, options=options)  # Pass the Service object to the constructor
     # Open the webpage
    driver.get(url)

    # Wait for the page to fully load
    time.sleep(5)  # Adjust the time depending on how long the page takes to load

    # Get the page source and parse it
    html = driver.page_source
    soup = BeautifulSoup(html, 'html.parser')

    # Close the WebDriver
    driver.quit()

    return soup

def scrape_with_selenium(url):
    # Set up the WebDriver using Service
    # Update the executable_path to the correct location of your chromedriver
    service = Service(executable_path=r'/path/to/chromedriver')  # Create a Service object
    # Add options if needed, such as headless mode
    options = webdriver.ChromeOptions()
    # options.add_argument('--headless')  # Uncomment to run in headless mode
    driver = webdriver.Chrome(service=service, options=options)  # Pass the Service object to the constructor
     # Open the webpage
    driver.get(url)

    # Wait for the page to fully load
    time.sleep(5)  # Adjust the time depending on how long the page takes to load

    # Get the page source and parse it
    html = driver.page_source
    soup = BeautifulSoup(html, 'html.parser')

    # Close the WebDriver
    driver.quit()

    return soup
def scrape_news_data_with_selenium(category_url):
    soup = scrape_with_selenium(category_url)

    # Base URL to append to relative article URLs
    base_url = "https://www.tbsnews.net"

    # List to store scraped data
    news_data = []

    # Find all articles in the specified section
    articles = soup.find_all('div', class_='views-field views-field-title')

    for article in articles:
        try:
            # Extract article title and URL
            title_tag = article.find('a')
            title = title_tag.get_text(strip=True)
            article_url = base_url + title_tag['href']

            # Optional: Add additional data extraction if available
            published_date = article.find_next('span', class_='field-content').get_text(strip=True) if article.find_next('span', class_='field-content') else 'No date available'

            # Add the scraped data to the list
            news_data.append({
                'title': title,
                'url': article_url,
                'published_date': published_date
            })
        except Exception as e:
            print(f"Error while scraping article: {e}")

    return news_data

# URL of the category page you want to scrape
category_url = 'https://www.tbsnews.net/bangla/%E0%A6%85%E0%A6%B0%E0%A7%8D%E0%A6%A5%E0%A6%A8%E0%A7%80%E0%A6%A4%E0%A6%BF'

# Scrape the data using Selenium
news_data = scrape_news_data_with_selenium(category_url)

# Save the data to JSON
with open('news_data.json', 'w', encoding='utf-8') as f:
    json.dump(news_data, f, ensure_ascii=False, indent=4)

print(f"Scraped {len(news_data)} articles and saved to news_data.json")

